{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJEzDG6DK2Q"
      },
      "source": [
        "## Addestramento di un modello personalizzato di rilevamento oggetti con TensorFlow Lite Model Maker\n",
        "\n",
        "In questo notebook Jupyter sarà effettuato l'addestramento e la valutazione di un modello personalizzato di rilevamento oggetti in grado di rilevare volti e testi. Il modello TensorFlow ottenuto dall'addestramento sarà convertito in un modello TensorFlow Lite. Quest'ultimo sarà importato nell'applicazione Android per eseguire il rilevamento di volti e scritte."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYjtwRZGBOI"
      },
      "source": [
        "## Preparazione\n",
        "\n",
        "### Installare i pacchetti richiesti\n",
        "\n",
        "Partiamo con l'installazione dei pacchetti richiesti:\n",
        "* **tflite-model-maker:** pacchetto del Model Maker, preso direttamente dalla sua [repository](https://github.com/tensorflow/examples/tree/master/tensorflow_examples/lite/model_maker) su GitHub. \n",
        "* **tflite-support:** la libreria pycocotools che sarà usata per la valutazione dei modelli."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35BJmtVpAP_n",
        "outputId": "a38b1fbe-3fa8-40fa-fa28-573281b4b10e"
      },
      "outputs": [],
      "source": [
        "!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prQ86DdtD317"
      },
      "source": [
        "Importiamo i pacchetti necessari."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l4QQTXHHATDS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3g6aQvXsD78P"
      },
      "source": [
        "### Preparazione del dataset\n",
        "\n",
        "Questo dataset contiene 25.000 immagini in cui ci sono i due oggetti da rilevare: visi e testi. \n",
        "\n",
        "Scarichiamo da Google Drive l'archivio zip del dataset, lo estraiamo e infine eliminiamo l'archivio zip per risparmiare spazio sul disco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AGg7D4JAV62",
        "outputId": "5963c39c-901f-4bc7-83fd-9e3b82c58ae7"
      },
      "outputs": [],
      "source": [
        "!gdown '1UVNMX8GBapg30CxnW7kJe0XEZsHUu0F0'\n",
        "!unzip -q dataset.zip\n",
        "!rm dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Addestramento del modello di rilevamento oggetti\n",
        "\n",
        "### Passo 1: Caricare il dataset\n",
        "\n",
        "* Le immagini in `train_data` sono usate per addestrare il modello di rilevamento oggetti.\n",
        "* Le immagini in `val_data` sono usate per controllare se il modello riesce a generalizzare bene analizzando immagini mai viste prima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiAahdsQAdT7"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/dataset/train',\n",
        "    '/content/dataset/train',\n",
        "    ['faccia', 'testo']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/dataset/validation',\n",
        "    '/content/dataset/validation',\n",
        "    ['faccia', 'testo']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Passo 2: Selezionare l'architettura del modello\n",
        "\n",
        "La EfficientDet-Lite[0-4] è una famiglia di modelli di rilevamento oggetti di tipo mobile/IoT-friendly derivati dell'architettura [EfficientDet](https://arxiv.org/abs/1911.09070).\n",
        "\n",
        "Di seguito, viene riportato un confronto tra le prestazioni di ciascun modello EfficientDet-Lite.\n",
        "\n",
        "|    Architettura    | Dimensione (MB)* | Latenza (ms)** | Precisione media*** |\n",
        "|--------------------|------------------|----------------|---------------------|\n",
        "| EfficientDet-Lite0 | 4.4              | 37             | 25.69%              |\n",
        "| EfficientDet-Lite1 | 5.8              | 49             | 30.55%              |\n",
        "| EfficientDet-Lite2 | 7.2              | 69             | 33.97%              |\n",
        "| EfficientDet-Lite3 | 11.4             | 116            | 37.70%              |\n",
        "| EfficientDet-Lite4 | 19.9             | 260            | 41.96%              |\n",
        "\n",
        "<i> * Dimensione dei modelli con quantizzazione full integer. <br/>\n",
        "** Latenza misurata su un Pixel 4 che usa 4 thread sulla CPU. <br/>\n",
        "*** Precisione media di tipo mAP (mean Average Precision) misurata con il validation set del dataset COCO 2017.\n",
        "</i>\n",
        "\n",
        "Nel nostro progetto abbiamo usato la EfficientDet-Lite0 per la sua velocità e la EfficientDet-Lite2 per una maggiore precisione."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "spec = object_detector.EfficientDetSpec(model_name='efficientdet-lite0', uri='https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1', hparams={'max_instances_per_image': 10000})\n",
        "\n",
        "#spec = object_detector.EfficientDetSpec(model_name='efficientdet-lite2', uri='https://tfhub.dev/tensorflow/efficientdet/lite2/feature-vector/1', hparams={'max_instances_per_image': 10000})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "### Passo 3: Addestrare il modello TensorFlow con il training set\n",
        "\n",
        "* `epochs = 30`: durante l'addestramento il training set sarà esaminato per 30 volte. E' importante stare attenti all'accuratezza della validation e fermarsi quando il valore di `val_loss` smette di diminuire per evitare overfitting.\n",
        "* `batch_size = 20`: saranno necessari 1.000 step per esaminare tutte le 20.000 immagini del training set.\n",
        "* `train_whole_model=True`: usato per il fine-tuning dell'intero modello, evitando di allenare solamente il layer head per migliorare l'accuratezza. Purtroppo, questa opzione dilata i tempi di addestramento.\n",
        "\n",
        "L'addestramento con Model Maker è basato sul transfer learning, ovvero sulla riconversione della rete preaddestrata (selezionata in `spec`) per riconoscere volti e testi. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MClfpsJAfda"
      },
      "outputs": [],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=20, train_whole_model=True, epochs=30, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Passo 4: Valutare il modello TensorFlow\n",
        "\n",
        "Dopo aver addestrato il modello di rilevamento oggetti utilizzando le immagini del training set, si utilizzano le immagini del validation set per valutare le sue prestazioni.\n",
        "\n",
        "In questo caso, la dimensione di default del batch è 64.\n",
        "\n",
        "Le metriche di valutazione sono le stesse usate da [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUqEpcYwAg8L"
      },
      "outputs": [],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Passo 5: Esportare come modello TensorFlow Lite\n",
        "\n",
        "Esportare il modello di rilevamento oggetti addestrato nel formato TensorFlow Lite. La tecnica di quantizzazione post-allenamento predefinita è la [quantizzazione full integer](https://www.tensorflow.org/lite/performance/post_training_integer_quant). Ciò consente al modello TensorFlow Lite di essere più piccolo e veloce quando sarà integrato nella nostra applicazione Android."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcUervW6nN6e"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='android.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Passo 6:  Valutare il modello TensorFlow Lite\n",
        "\n",
        "Diversi fattori possono influenzare la precisione del modello durante l'esportazione in TensorFlow Lite:\n",
        "* Usiamo la [quantizzazione fill-integer](https://www.tensorflow.org/lite/performance/model_optimization) che converte i pesi e le operazioni della rete neurale da Float32 a Int8; riducendo la dimensione del modello fino a 4 volte, velocizzandolo di circa 3 volte ma perdendo anche precisione.\n",
        "* Nel post-processing, il modello TensorFlow utilizza per ogni classe il [NMS (Non-Max Supression)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH); mentre il modello TensorFlow Lite usa il NMS globalmente con il conseguente aumento di velocità a discapito dell'accuratezza.\n",
        "* Keras è in grado di rilevare massimo 100 oggetti, mentre TensorFlow Lite solamente 25."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqV48TKBt1hG"
      },
      "outputs": [],
      "source": [
        "model.evaluate_tflite('android.tflite', val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_G22zldraNH"
      },
      "source": [
        "### Passo 7:  Scaricare il modello TensorFlow Lite\n",
        "\n",
        "Scaricare il modello TensorFlow Lite ottenuto e importarlo nell'applicazione Android."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khlM5LuGraci"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('android.tflite')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
